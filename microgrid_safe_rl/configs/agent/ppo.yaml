algo: ppo
ppo:
  policy: MlpPolicy
  gamma: 0.995
  gae_lambda: 0.95
  n_steps: 4096            # per env; raise/lower with GPU/CPU RAM
  batch_size: 1024
  n_epochs: 10
  learning_rate: 3.0e-4
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  normalize_advantage: true
  use_sde: false
  policy_kwargs:
    activation_fn: relu
    net_arch:
      pi:  [256, 256]
      vf:  [256, 256]
